{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1) a)\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#charger la base de données\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "#affichage de la base de données\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[9]) #index est le numéro de l’image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre d'exemples : 1797, Dimension : 64\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf1 = MLPClassifier(hidden_layer_sizes=5, activation='tanh', solver='sgd', batch_size=1, alpha=0, learning_rate='adaptive', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.03557649\n",
      "Iteration 2, loss = 1.78042136\n",
      "Iteration 3, loss = 1.66392595\n",
      "Iteration 4, loss = 1.60771162\n",
      "Iteration 5, loss = 1.66323682\n",
      "Iteration 6, loss = 1.60300841\n",
      "Iteration 7, loss = 1.60793314\n",
      "Iteration 8, loss = 1.57422164\n",
      "Iteration 9, loss = 1.57552463\n",
      "Iteration 10, loss = 1.51521681\n",
      "Iteration 11, loss = 1.55577472\n",
      "Iteration 12, loss = 1.53023630\n",
      "Iteration 13, loss = 1.53407147\n",
      "Iteration 14, loss = 1.52470131\n",
      "Iteration 15, loss = 1.55328927\n",
      "Iteration 16, loss = 1.52969923\n",
      "Iteration 17, loss = 1.55201270\n",
      "Iteration 18, loss = 1.54309919\n",
      "Iteration 19, loss = 1.50364452\n",
      "Iteration 20, loss = 1.50328619\n",
      "Iteration 21, loss = 1.51997092\n",
      "Iteration 22, loss = 1.49974228\n",
      "Iteration 23, loss = 1.49927446\n",
      "Iteration 24, loss = 1.49495125\n",
      "Iteration 25, loss = 1.50947960\n",
      "Iteration 26, loss = 1.47605130\n",
      "Iteration 27, loss = 1.46124484\n",
      "Iteration 28, loss = 1.44961600\n",
      "Iteration 29, loss = 1.48110652\n",
      "Iteration 30, loss = 1.52664198\n",
      "Iteration 31, loss = 1.49179025\n",
      "Iteration 32, loss = 1.48319138\n",
      "Iteration 33, loss = 1.42985056\n",
      "Iteration 34, loss = 1.51378268\n",
      "Iteration 35, loss = 1.48319846\n",
      "Iteration 36, loss = 1.43914217\n",
      "Iteration 37, loss = 1.53961820\n",
      "Iteration 38, loss = 1.46486324\n",
      "Iteration 39, loss = 1.45268301\n",
      "Iteration 40, loss = 1.42512096\n",
      "Iteration 41, loss = 1.40433723\n",
      "Iteration 42, loss = 1.42609511\n",
      "Iteration 43, loss = 1.40998963\n",
      "Iteration 44, loss = 1.43546120\n",
      "Iteration 45, loss = 1.50279696\n",
      "Iteration 46, loss = 1.45106436\n",
      "Iteration 47, loss = 1.43331807\n",
      "Iteration 48, loss = 1.45700160\n",
      "Iteration 49, loss = 1.52046544\n",
      "Iteration 50, loss = 1.51831241\n",
      "Iteration 51, loss = 1.49917275\n",
      "Iteration 52, loss = 1.42816329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 53, loss = 1.38846807\n",
      "Iteration 54, loss = 1.38390884\n",
      "Iteration 55, loss = 1.37994203\n",
      "Iteration 56, loss = 1.38727394\n",
      "Iteration 57, loss = 1.37745214\n",
      "Iteration 58, loss = 1.37748214\n",
      "Iteration 59, loss = 1.36941118\n",
      "Iteration 60, loss = 1.37880077\n",
      "Iteration 61, loss = 1.38236420\n",
      "Iteration 62, loss = 1.37152605\n",
      "Iteration 63, loss = 1.35540097\n",
      "Iteration 64, loss = 1.36474963\n",
      "Iteration 65, loss = 1.38122811\n",
      "Iteration 66, loss = 1.35410444\n",
      "Iteration 67, loss = 1.34588824\n",
      "Iteration 68, loss = 1.34398477\n",
      "Iteration 69, loss = 1.34843629\n",
      "Iteration 70, loss = 1.34836831\n",
      "Iteration 71, loss = 1.34056918\n",
      "Iteration 72, loss = 1.35415673\n",
      "Iteration 73, loss = 1.35303691\n",
      "Iteration 74, loss = 1.35057864\n",
      "Iteration 75, loss = 1.34719646\n",
      "Iteration 76, loss = 1.34947513\n",
      "Iteration 77, loss = 1.34621368\n",
      "Iteration 78, loss = 1.33398468\n",
      "Iteration 79, loss = 1.33080099\n",
      "Iteration 80, loss = 1.33311391\n",
      "Iteration 81, loss = 1.34226220\n",
      "Iteration 82, loss = 1.35298639\n",
      "Iteration 83, loss = 1.35394625\n",
      "Iteration 84, loss = 1.34649636\n",
      "Iteration 85, loss = 1.34469311\n",
      "Iteration 86, loss = 1.32999329\n",
      "Iteration 87, loss = 1.34801197\n",
      "Iteration 88, loss = 1.34686168\n",
      "Iteration 89, loss = 1.36169519\n",
      "Iteration 90, loss = 1.35657470\n",
      "Iteration 91, loss = 1.33798144\n",
      "Iteration 92, loss = 1.35852172\n",
      "Iteration 93, loss = 1.33743297\n",
      "Iteration 94, loss = 1.33612539\n",
      "Iteration 95, loss = 1.34352260\n",
      "Iteration 96, loss = 1.32289413\n",
      "Iteration 97, loss = 1.31983305\n",
      "Iteration 98, loss = 1.32760742\n",
      "Iteration 99, loss = 1.33678799\n",
      "Iteration 100, loss = 1.31548458\n",
      "Iteration 101, loss = 1.31845794\n",
      "Iteration 102, loss = 1.32688184\n",
      "Iteration 103, loss = 1.31949226\n",
      "Iteration 104, loss = 1.30815523\n",
      "Iteration 105, loss = 1.33335066\n",
      "Iteration 106, loss = 1.31904203\n",
      "Iteration 107, loss = 1.30873843\n",
      "Iteration 108, loss = 1.30884133\n",
      "Iteration 109, loss = 1.30378115\n",
      "Iteration 110, loss = 1.32726504\n",
      "Iteration 111, loss = 1.30848348\n",
      "Iteration 112, loss = 1.28064492\n",
      "Iteration 113, loss = 1.27515895\n",
      "Iteration 114, loss = 1.27083826\n",
      "Iteration 115, loss = 1.25113869\n",
      "Iteration 116, loss = 1.24404072\n",
      "Iteration 117, loss = 1.21500482\n",
      "Iteration 118, loss = 1.21553841\n",
      "Iteration 119, loss = 1.20079044\n",
      "Iteration 120, loss = 1.18870795\n",
      "Iteration 121, loss = 1.17839787\n",
      "Iteration 122, loss = 1.16419101\n",
      "Iteration 123, loss = 1.17131659\n",
      "Iteration 124, loss = 1.16811836\n",
      "Iteration 125, loss = 1.14728703\n",
      "Iteration 126, loss = 1.15070158\n",
      "Iteration 127, loss = 1.14062811\n",
      "Iteration 128, loss = 1.16318040\n",
      "Iteration 129, loss = 1.15042485\n",
      "Iteration 130, loss = 1.18306178\n",
      "Iteration 131, loss = 1.13215114\n",
      "Iteration 132, loss = 1.13569126\n",
      "Iteration 133, loss = 1.12249789\n",
      "Iteration 134, loss = 1.11548313\n",
      "Iteration 135, loss = 1.12579642\n",
      "Iteration 136, loss = 1.12211131\n",
      "Iteration 137, loss = 1.11023759\n",
      "Iteration 138, loss = 1.12651360\n",
      "Iteration 139, loss = 1.12202809\n",
      "Iteration 140, loss = 1.11411807\n",
      "Iteration 141, loss = 1.16157277\n",
      "Iteration 142, loss = 1.10222247\n",
      "Iteration 143, loss = 1.11286530\n",
      "Iteration 144, loss = 1.13465643\n",
      "Iteration 145, loss = 1.10091622\n",
      "Iteration 146, loss = 1.08943569\n",
      "Iteration 147, loss = 1.10169289\n",
      "Iteration 148, loss = 1.09457997\n",
      "Iteration 149, loss = 1.08032435\n",
      "Iteration 150, loss = 1.10701949\n",
      "Iteration 151, loss = 1.06477768\n",
      "Iteration 152, loss = 1.03244781\n",
      "Iteration 153, loss = 1.00779747\n",
      "Iteration 154, loss = 1.00526455\n",
      "Iteration 155, loss = 0.97462300\n",
      "Iteration 156, loss = 0.98148813\n",
      "Iteration 157, loss = 0.95811959\n",
      "Iteration 158, loss = 0.96070590\n",
      "Iteration 159, loss = 0.94762081\n",
      "Iteration 160, loss = 0.95734981\n",
      "Iteration 161, loss = 0.93694526\n",
      "Iteration 162, loss = 0.93497792\n",
      "Iteration 163, loss = 0.95046379\n",
      "Iteration 164, loss = 0.95828283\n",
      "Iteration 165, loss = 0.93782976\n",
      "Iteration 166, loss = 0.91770102\n",
      "Iteration 167, loss = 0.93338018\n",
      "Iteration 168, loss = 0.90604178\n",
      "Iteration 169, loss = 0.92214587\n",
      "Iteration 170, loss = 0.91367614\n",
      "Iteration 171, loss = 0.93575890\n",
      "Iteration 172, loss = 0.91030058\n",
      "Iteration 173, loss = 0.90535629\n",
      "Iteration 174, loss = 0.90393976\n",
      "Iteration 175, loss = 0.91263978\n",
      "Iteration 176, loss = 0.90536701\n",
      "Iteration 177, loss = 0.91844320\n",
      "Iteration 178, loss = 0.94311076\n",
      "Iteration 179, loss = 0.91606068\n",
      "Iteration 180, loss = 0.90415869\n",
      "Iteration 181, loss = 0.94968393\n",
      "Iteration 182, loss = 0.91597151\n",
      "Iteration 183, loss = 0.91180249\n",
      "Iteration 184, loss = 0.91192134\n",
      "Iteration 185, loss = 0.90673486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 186, loss = 0.89234519\n",
      "Iteration 187, loss = 0.88466275\n",
      "Iteration 188, loss = 0.88409991\n",
      "Iteration 189, loss = 0.88379033\n",
      "Iteration 190, loss = 0.88319066\n",
      "Iteration 191, loss = 0.88281536\n",
      "Iteration 192, loss = 0.88261212\n",
      "Iteration 193, loss = 0.88218642\n",
      "Iteration 194, loss = 0.88194839\n",
      "Iteration 195, loss = 0.88109194\n",
      "Iteration 196, loss = 0.88144403\n",
      "Iteration 197, loss = 0.87945111\n",
      "Iteration 198, loss = 0.87933578\n",
      "Iteration 199, loss = 0.87813535\n",
      "Iteration 200, loss = 0.87766213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas Prst\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 8, 6, 4, 6, 8, 8, 5, 4, 7, 5, 4, 5, 8, 8, 8, 4, 4, 5, 2, 2, 7,\n",
       "       5, 4, 6, 2, 4, 6, 6, 6, 7, 6, 8, 4, 2, 2, 7, 7, 6, 4, 6, 6, 7, 8,\n",
       "       6, 7, 8, 9, 4, 8, 8, 6, 6, 4, 2, 8, 9, 2, 6, 4, 2, 6, 8, 6, 6, 2,\n",
       "       7, 8, 2, 5, 7, 4, 8, 8, 8, 6, 8, 6, 8, 5, 2, 5, 4, 7, 2, 2, 8, 2,\n",
       "       8, 5, 6, 8, 6, 9, 6, 6, 2, 5, 2, 5, 6, 6, 7, 2, 8, 6, 4, 8, 6, 7,\n",
       "       5, 6, 8, 6, 4, 6, 8, 6, 6, 2, 8, 9, 2, 2, 8, 8, 8, 8, 5, 7, 2, 5,\n",
       "       6, 8, 8, 8, 2, 6, 4, 8, 2, 4, 2, 5, 8, 8, 4, 4, 2, 8, 8, 2, 6, 8,\n",
       "       8, 8, 2, 2, 5, 6, 2, 6, 8, 6, 2, 6, 8, 2, 8, 2, 2, 8, 2, 6, 8, 7,\n",
       "       5, 7, 2, 6, 6, 2, 5, 8, 2, 4, 6, 2, 6, 8, 9, 8, 7, 8, 8, 6, 8, 6,\n",
       "       2, 2, 8, 8, 8, 7, 2, 5, 8, 4, 8, 8, 2, 4, 5, 8, 8, 7, 6, 7, 8, 2,\n",
       "       2, 4, 2, 6, 6, 4, 6, 6, 2, 4, 5, 2, 2, 8, 4, 8, 7, 5, 6, 8, 6, 2,\n",
       "       6, 2, 6, 8, 5, 7, 8, 2, 7, 8, 5, 2, 2, 6, 2, 6, 2, 8, 6, 8, 4, 2,\n",
       "       6, 9, 7, 8, 2, 4, 2, 6, 8, 6, 5, 8, 8, 8, 8, 7, 5, 8, 6, 8, 6, 2,\n",
       "       8, 9, 6, 2, 4, 8, 8, 8, 5, 5, 8, 8, 6, 6, 8, 4, 6, 8, 2, 7, 2, 6,\n",
       "       6, 8, 8, 6, 2, 6, 2, 2, 6, 5, 6, 2, 2, 7, 2, 7, 5, 8, 2, 8, 9, 8,\n",
       "       6, 4, 8, 4, 6, 5, 6, 8, 9, 7, 6, 8, 5, 5, 8, 8, 8, 4, 8, 6, 8, 8,\n",
       "       6, 2, 6, 7, 8, 8, 6, 6, 5, 8, 5, 6, 2, 6, 7, 6, 8, 9, 7, 2, 4, 5,\n",
       "       5, 5, 2, 7, 2, 6, 8, 6, 2, 6, 4, 4, 8, 6, 4, 6, 5, 6, 8, 2, 8, 2,\n",
       "       4, 8, 8, 2, 2, 6, 8, 8, 5, 5, 8, 2, 6, 8, 8, 5, 8, 5, 7, 2, 4, 7,\n",
       "       0, 6, 2, 5, 5, 6, 6, 6, 8, 2, 6, 6, 1, 4, 4, 4, 2, 8, 6, 6, 2, 2,\n",
       "       8, 2, 4, 2, 6, 2, 8, 2, 6, 8, 5, 4, 2, 8, 2, 8, 6, 2, 8, 8, 6, 2,\n",
       "       4, 2, 1, 8, 2, 6, 4, 7, 8, 8, 5, 7, 2, 6, 7, 8, 7, 6, 5, 7, 4, 2,\n",
       "       6, 2, 6, 2, 8, 6, 8, 8, 7, 8, 2, 4, 8, 6, 2, 4, 8, 2, 2, 8, 6, 8,\n",
       "       6, 8, 4, 6, 9, 8, 8, 2, 2, 8, 8, 2, 5, 2, 2, 7, 7, 8, 2, 6, 2, 6,\n",
       "       8, 2, 8, 5, 7, 6, 7, 2, 5, 9, 5, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train, y_train)\n",
    "clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32507735\n",
      "Iteration 2, loss = 2.12109525\n",
      "Iteration 3, loss = 2.01226772\n",
      "Iteration 4, loss = 1.96445867\n",
      "Iteration 5, loss = 1.93357548\n",
      "Iteration 6, loss = 1.91193825\n",
      "Iteration 7, loss = 1.90880032\n",
      "Iteration 8, loss = 1.87334329\n",
      "Iteration 9, loss = 1.84530107\n",
      "Iteration 10, loss = 1.82907192\n",
      "Iteration 11, loss = 1.81225252\n",
      "Iteration 12, loss = 1.80934971\n",
      "Iteration 13, loss = 1.80526771\n",
      "Iteration 14, loss = 1.80462334\n",
      "Iteration 15, loss = 1.80432606\n",
      "Iteration 16, loss = 1.79604116\n",
      "Iteration 17, loss = 1.79335935\n",
      "Iteration 18, loss = 1.79500677\n",
      "Iteration 19, loss = 1.78110825\n",
      "Iteration 20, loss = 1.78102037\n",
      "Iteration 21, loss = 1.77888839\n",
      "Iteration 22, loss = 1.77357416\n",
      "Iteration 23, loss = 1.77503483\n",
      "Iteration 24, loss = 1.77696785\n",
      "Iteration 25, loss = 1.76789330\n",
      "Iteration 26, loss = 1.77287490\n",
      "Iteration 27, loss = 1.77045374\n",
      "Iteration 28, loss = 1.76670798\n",
      "Iteration 29, loss = 1.76748707\n",
      "Iteration 30, loss = 1.76842058\n",
      "Iteration 31, loss = 1.77046636\n",
      "Iteration 32, loss = 1.76469691\n",
      "Iteration 33, loss = 1.76897440\n",
      "Iteration 34, loss = 1.76736697\n",
      "Iteration 35, loss = 1.76227320\n",
      "Iteration 36, loss = 1.77229872\n",
      "Iteration 37, loss = 1.76937772\n",
      "Iteration 38, loss = 1.76742145\n",
      "Iteration 39, loss = 1.76427375\n",
      "Iteration 40, loss = 1.76359446\n",
      "Iteration 41, loss = 1.75743430\n",
      "Iteration 42, loss = 1.76011483\n",
      "Iteration 43, loss = 1.76309268\n",
      "Iteration 44, loss = 1.75785713\n",
      "Iteration 45, loss = 1.77069281\n",
      "Iteration 46, loss = 1.76433889\n",
      "Iteration 47, loss = 1.76037220\n",
      "Iteration 48, loss = 1.76332170\n",
      "Iteration 49, loss = 1.76109805\n",
      "Iteration 50, loss = 1.76174578\n",
      "Iteration 51, loss = 1.75251455\n",
      "Iteration 52, loss = 1.75996260\n",
      "Iteration 53, loss = 1.76126268\n",
      "Iteration 54, loss = 1.75327685\n",
      "Iteration 55, loss = 1.75030940\n",
      "Iteration 56, loss = 1.75593349\n",
      "Iteration 57, loss = 1.75849229\n",
      "Iteration 58, loss = 1.74123768\n",
      "Iteration 59, loss = 1.75883543\n",
      "Iteration 60, loss = 1.75191122\n",
      "Iteration 61, loss = 1.75739641\n",
      "Iteration 62, loss = 1.74418799\n",
      "Iteration 63, loss = 1.75605202\n",
      "Iteration 64, loss = 1.75395570\n",
      "Iteration 65, loss = 1.75326310\n",
      "Iteration 66, loss = 1.74972351\n",
      "Iteration 67, loss = 1.75307854\n",
      "Iteration 68, loss = 1.76474364\n",
      "Iteration 69, loss = 1.75760670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25603689\n",
      "Iteration 2, loss = 1.97526538\n",
      "Iteration 3, loss = 1.81670968\n",
      "Iteration 4, loss = 1.68777358\n",
      "Iteration 5, loss = 1.60500248\n",
      "Iteration 6, loss = 1.53461710\n",
      "Iteration 7, loss = 1.48801947\n",
      "Iteration 8, loss = 1.45477803\n",
      "Iteration 9, loss = 1.43360452\n",
      "Iteration 10, loss = 1.41152111\n",
      "Iteration 11, loss = 1.40169253\n",
      "Iteration 12, loss = 1.37571362\n",
      "Iteration 13, loss = 1.37456371\n",
      "Iteration 14, loss = 1.35338837\n",
      "Iteration 15, loss = 1.35375037\n",
      "Iteration 16, loss = 1.33482294\n",
      "Iteration 17, loss = 1.33506087\n",
      "Iteration 18, loss = 1.33144769\n",
      "Iteration 19, loss = 1.31210896\n",
      "Iteration 20, loss = 1.31364059\n",
      "Iteration 21, loss = 1.29668317\n",
      "Iteration 22, loss = 1.30687433\n",
      "Iteration 23, loss = 1.31240048\n",
      "Iteration 24, loss = 1.29269642\n",
      "Iteration 25, loss = 1.28451799\n",
      "Iteration 26, loss = 1.29736726\n",
      "Iteration 27, loss = 1.30085626\n",
      "Iteration 28, loss = 1.28196836\n",
      "Iteration 29, loss = 1.28427896\n",
      "Iteration 30, loss = 1.28313371\n",
      "Iteration 31, loss = 1.29275430\n",
      "Iteration 32, loss = 1.27768864\n",
      "Iteration 33, loss = 1.28521457\n",
      "Iteration 34, loss = 1.27164710\n",
      "Iteration 35, loss = 1.26574289\n",
      "Iteration 36, loss = 1.25948907\n",
      "Iteration 37, loss = 1.25308175\n",
      "Iteration 38, loss = 1.26299036\n",
      "Iteration 39, loss = 1.25887800\n",
      "Iteration 40, loss = 1.23794689\n",
      "Iteration 41, loss = 1.25846820\n",
      "Iteration 42, loss = 1.24481304\n",
      "Iteration 43, loss = 1.25954978\n",
      "Iteration 44, loss = 1.24958773\n",
      "Iteration 45, loss = 1.23960693\n",
      "Iteration 46, loss = 1.24596014\n",
      "Iteration 47, loss = 1.24200215\n",
      "Iteration 48, loss = 1.25043184\n",
      "Iteration 49, loss = 1.25116417\n",
      "Iteration 50, loss = 1.25696847\n",
      "Iteration 51, loss = 1.24408662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30818310\n",
      "Iteration 2, loss = 2.13338641\n",
      "Iteration 3, loss = 2.01775423\n",
      "Iteration 4, loss = 1.78149197\n",
      "Iteration 5, loss = 1.64736656\n",
      "Iteration 6, loss = 1.58053752\n",
      "Iteration 7, loss = 1.52244220\n",
      "Iteration 8, loss = 1.47667633\n",
      "Iteration 9, loss = 1.43089860\n",
      "Iteration 10, loss = 1.30037740\n",
      "Iteration 11, loss = 1.19202374\n",
      "Iteration 12, loss = 1.13313475\n",
      "Iteration 13, loss = 1.08279414\n",
      "Iteration 14, loss = 1.06902541\n",
      "Iteration 15, loss = 1.04802017\n",
      "Iteration 16, loss = 1.00677885\n",
      "Iteration 17, loss = 0.99347249\n",
      "Iteration 18, loss = 1.00187552\n",
      "Iteration 19, loss = 0.96736486\n",
      "Iteration 20, loss = 0.94723420\n",
      "Iteration 21, loss = 0.95044286\n",
      "Iteration 22, loss = 0.94091428\n",
      "Iteration 23, loss = 0.93950507\n",
      "Iteration 24, loss = 0.93426573\n",
      "Iteration 25, loss = 0.91254228\n",
      "Iteration 26, loss = 0.88703527\n",
      "Iteration 27, loss = 0.90886787\n",
      "Iteration 28, loss = 0.90669588\n",
      "Iteration 29, loss = 0.91290845\n",
      "Iteration 30, loss = 0.89121662\n",
      "Iteration 31, loss = 0.88526646\n",
      "Iteration 32, loss = 0.88709636\n",
      "Iteration 33, loss = 0.89706413\n",
      "Iteration 34, loss = 0.86747560\n",
      "Iteration 35, loss = 0.86653874\n",
      "Iteration 36, loss = 0.86864591\n",
      "Iteration 37, loss = 0.86670433\n",
      "Iteration 38, loss = 0.87514887\n",
      "Iteration 39, loss = 0.87352541\n",
      "Iteration 40, loss = 0.87425670\n",
      "Iteration 41, loss = 0.87154885\n",
      "Iteration 42, loss = 0.86313935\n",
      "Iteration 43, loss = 0.85348904\n",
      "Iteration 44, loss = 0.84903895\n",
      "Iteration 45, loss = 0.86744043\n",
      "Iteration 46, loss = 0.86246915\n",
      "Iteration 47, loss = 0.86371227\n",
      "Iteration 48, loss = 0.86907899\n",
      "Iteration 49, loss = 0.85059335\n",
      "Iteration 50, loss = 0.84753294\n",
      "Iteration 51, loss = 0.88531735\n",
      "Iteration 52, loss = 0.84169802\n",
      "Iteration 53, loss = 0.86046945\n",
      "Iteration 54, loss = 0.85681992\n",
      "Iteration 55, loss = 0.86321033\n",
      "Iteration 56, loss = 0.84905902\n",
      "Iteration 57, loss = 0.83546220\n",
      "Iteration 58, loss = 0.84915535\n",
      "Iteration 59, loss = 0.85026538\n",
      "Iteration 60, loss = 0.83555565\n",
      "Iteration 61, loss = 0.83142109\n",
      "Iteration 62, loss = 0.84516811\n",
      "Iteration 63, loss = 0.85148252\n",
      "Iteration 64, loss = 0.84073773\n",
      "Iteration 65, loss = 0.83544941\n",
      "Iteration 66, loss = 0.84363875\n",
      "Iteration 67, loss = 0.82917847\n",
      "Iteration 68, loss = 0.82427022\n",
      "Iteration 69, loss = 0.84252469\n",
      "Iteration 70, loss = 0.83664280\n",
      "Iteration 71, loss = 0.82419316\n",
      "Iteration 72, loss = 0.84076684\n",
      "Iteration 73, loss = 0.81197033\n",
      "Iteration 74, loss = 0.83030306\n",
      "Iteration 75, loss = 0.83437446\n",
      "Iteration 76, loss = 0.83021498\n",
      "Iteration 77, loss = 0.81994679\n",
      "Iteration 78, loss = 0.82176056\n",
      "Iteration 79, loss = 0.83841342\n",
      "Iteration 80, loss = 0.83895241\n",
      "Iteration 81, loss = 0.83192211\n",
      "Iteration 82, loss = 0.81993699\n",
      "Iteration 83, loss = 0.80084386\n",
      "Iteration 84, loss = 0.84310682\n",
      "Iteration 85, loss = 0.82102587\n",
      "Iteration 86, loss = 0.81761746\n",
      "Iteration 87, loss = 0.81595088\n",
      "Iteration 88, loss = 0.84096274\n",
      "Iteration 89, loss = 0.81821527\n",
      "Iteration 90, loss = 0.81740858\n",
      "Iteration 91, loss = 0.82312600\n",
      "Iteration 92, loss = 0.85280153\n",
      "Iteration 93, loss = 0.82784811\n",
      "Iteration 94, loss = 0.82418434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27005733\n",
      "Iteration 2, loss = 1.87562333\n",
      "Iteration 3, loss = 1.67842788\n",
      "Iteration 4, loss = 1.52705932\n",
      "Iteration 5, loss = 1.38128180\n",
      "Iteration 6, loss = 1.25681902\n",
      "Iteration 7, loss = 1.18420438\n",
      "Iteration 8, loss = 1.11910549\n",
      "Iteration 9, loss = 1.08885674\n",
      "Iteration 10, loss = 1.05200588\n",
      "Iteration 11, loss = 1.01757519\n",
      "Iteration 12, loss = 1.00286596\n",
      "Iteration 13, loss = 0.98014293\n",
      "Iteration 14, loss = 0.96597973\n",
      "Iteration 15, loss = 0.94694410\n",
      "Iteration 16, loss = 0.91757793\n",
      "Iteration 17, loss = 0.89931400\n",
      "Iteration 18, loss = 0.88249525\n",
      "Iteration 19, loss = 0.88477638\n",
      "Iteration 20, loss = 0.84183423\n",
      "Iteration 21, loss = 0.84111160\n",
      "Iteration 22, loss = 0.81334094\n",
      "Iteration 23, loss = 0.82548340\n",
      "Iteration 24, loss = 0.82988207\n",
      "Iteration 25, loss = 0.79284399\n",
      "Iteration 26, loss = 0.80819187\n",
      "Iteration 27, loss = 0.82001985\n",
      "Iteration 28, loss = 0.79720190\n",
      "Iteration 29, loss = 0.81989705\n",
      "Iteration 30, loss = 0.79293977\n",
      "Iteration 31, loss = 0.78787014\n",
      "Iteration 32, loss = 0.78901900\n",
      "Iteration 33, loss = 0.78559109\n",
      "Iteration 34, loss = 0.78114524\n",
      "Iteration 35, loss = 0.77708960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.75817121\n",
      "Iteration 37, loss = 0.77124899\n",
      "Iteration 38, loss = 0.78592336\n",
      "Iteration 39, loss = 0.77792100\n",
      "Iteration 40, loss = 0.77637256\n",
      "Iteration 41, loss = 0.76653648\n",
      "Iteration 42, loss = 0.75757226\n",
      "Iteration 43, loss = 0.75412909\n",
      "Iteration 44, loss = 0.76551054\n",
      "Iteration 45, loss = 0.76146910\n",
      "Iteration 46, loss = 0.76256662\n",
      "Iteration 47, loss = 0.75572647\n",
      "Iteration 48, loss = 0.74446664\n",
      "Iteration 49, loss = 0.77347190\n",
      "Iteration 50, loss = 0.74773202\n",
      "Iteration 51, loss = 0.74794642\n",
      "Iteration 52, loss = 0.75798407\n",
      "Iteration 53, loss = 0.76168013\n",
      "Iteration 54, loss = 0.75196684\n",
      "Iteration 55, loss = 0.74996076\n",
      "Iteration 56, loss = 0.72796133\n",
      "Iteration 57, loss = 0.76021332\n",
      "Iteration 58, loss = 0.72605961\n",
      "Iteration 59, loss = 0.75621962\n",
      "Iteration 60, loss = 0.75312626\n",
      "Iteration 61, loss = 0.73366104\n",
      "Iteration 62, loss = 0.73673480\n",
      "Iteration 63, loss = 0.75559593\n",
      "Iteration 64, loss = 0.74815221\n",
      "Iteration 65, loss = 0.75493783\n",
      "Iteration 66, loss = 0.72494666\n",
      "Iteration 67, loss = 0.72753461\n",
      "Iteration 68, loss = 0.74900396\n",
      "Iteration 69, loss = 0.73510875\n",
      "Iteration 70, loss = 0.72829038\n",
      "Iteration 71, loss = 0.73087859\n",
      "Iteration 72, loss = 0.73597781\n",
      "Iteration 73, loss = 0.75113565\n",
      "Iteration 74, loss = 0.72580968\n",
      "Iteration 75, loss = 0.73666215\n",
      "Iteration 76, loss = 0.74865459\n",
      "Iteration 77, loss = 0.74438480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07579598\n",
      "Iteration 2, loss = 1.46027632\n",
      "Iteration 3, loss = 1.13005220\n",
      "Iteration 4, loss = 0.93464869\n",
      "Iteration 5, loss = 0.81494254\n",
      "Iteration 6, loss = 0.72481249\n",
      "Iteration 7, loss = 0.64682662\n",
      "Iteration 8, loss = 0.57353361\n",
      "Iteration 9, loss = 0.53565418\n",
      "Iteration 10, loss = 0.49822025\n",
      "Iteration 11, loss = 0.47147969\n",
      "Iteration 12, loss = 0.42304993\n",
      "Iteration 13, loss = 0.41863632\n",
      "Iteration 14, loss = 0.41352934\n",
      "Iteration 15, loss = 0.39448495\n",
      "Iteration 16, loss = 0.35966563\n",
      "Iteration 17, loss = 0.33590914\n",
      "Iteration 18, loss = 0.33868653\n",
      "Iteration 19, loss = 0.33894280\n",
      "Iteration 20, loss = 0.32642876\n",
      "Iteration 21, loss = 0.30138147\n",
      "Iteration 22, loss = 0.32012509\n",
      "Iteration 23, loss = 0.30260084\n",
      "Iteration 24, loss = 0.29336027\n",
      "Iteration 25, loss = 0.28229350\n",
      "Iteration 26, loss = 0.28805999\n",
      "Iteration 27, loss = 0.26046249\n",
      "Iteration 28, loss = 0.27985915\n",
      "Iteration 29, loss = 0.26508331\n",
      "Iteration 30, loss = 0.26143652\n",
      "Iteration 31, loss = 0.25185896\n",
      "Iteration 32, loss = 0.29863877\n",
      "Iteration 33, loss = 0.25898740\n",
      "Iteration 34, loss = 0.22986567\n",
      "Iteration 35, loss = 0.23791611\n",
      "Iteration 36, loss = 0.23344091\n",
      "Iteration 37, loss = 0.25104916\n",
      "Iteration 38, loss = 0.22781716\n",
      "Iteration 39, loss = 0.24232341\n",
      "Iteration 40, loss = 0.24141682\n",
      "Iteration 41, loss = 0.23709197\n",
      "Iteration 42, loss = 0.21260967\n",
      "Iteration 43, loss = 0.21868309\n",
      "Iteration 44, loss = 0.25380765\n",
      "Iteration 45, loss = 0.19594687\n",
      "Iteration 46, loss = 0.19951688\n",
      "Iteration 47, loss = 0.22114737\n",
      "Iteration 48, loss = 0.20355116\n",
      "Iteration 49, loss = 0.18109844\n",
      "Iteration 50, loss = 0.21336364\n",
      "Iteration 51, loss = 0.22238936\n",
      "Iteration 52, loss = 0.19554061\n",
      "Iteration 53, loss = 0.22545450\n",
      "Iteration 54, loss = 0.19560522\n",
      "Iteration 55, loss = 0.19586098\n",
      "Iteration 56, loss = 0.19909831\n",
      "Iteration 57, loss = 0.20608025\n",
      "Iteration 58, loss = 0.22334822\n",
      "Iteration 59, loss = 0.18919660\n",
      "Iteration 60, loss = 0.19795952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.78937812\n",
      "Iteration 2, loss = 1.03154458\n",
      "Iteration 3, loss = 0.67146799\n",
      "Iteration 4, loss = 0.50442105\n",
      "Iteration 5, loss = 0.39545584\n",
      "Iteration 6, loss = 0.32824553\n",
      "Iteration 7, loss = 0.28900094\n",
      "Iteration 8, loss = 0.25761637\n",
      "Iteration 9, loss = 0.22869778\n",
      "Iteration 10, loss = 0.20565479\n",
      "Iteration 11, loss = 0.18853259\n",
      "Iteration 12, loss = 0.16314564\n",
      "Iteration 13, loss = 0.15012523\n",
      "Iteration 14, loss = 0.13927528\n",
      "Iteration 15, loss = 0.14424752\n",
      "Iteration 16, loss = 0.10873463\n",
      "Iteration 17, loss = 0.13465438\n",
      "Iteration 18, loss = 0.09043863\n",
      "Iteration 19, loss = 0.09133761\n",
      "Iteration 20, loss = 0.09054564\n",
      "Iteration 21, loss = 0.09076707\n",
      "Iteration 22, loss = 0.08620711\n",
      "Iteration 23, loss = 0.08392102\n",
      "Iteration 24, loss = 0.07709021\n",
      "Iteration 25, loss = 0.08048299\n",
      "Iteration 26, loss = 0.06883585\n",
      "Iteration 27, loss = 0.08739704\n",
      "Iteration 28, loss = 0.06991709\n",
      "Iteration 29, loss = 0.05672272\n",
      "Iteration 30, loss = 0.05858417\n",
      "Iteration 31, loss = 0.06024888\n",
      "Iteration 32, loss = 0.07042920\n",
      "Iteration 33, loss = 0.05187780\n",
      "Iteration 34, loss = 0.06112632\n",
      "Iteration 35, loss = 0.05518144\n",
      "Iteration 36, loss = 0.06137980\n",
      "Iteration 37, loss = 0.06575710\n",
      "Iteration 38, loss = 0.05125381\n",
      "Iteration 39, loss = 0.03769254\n",
      "Iteration 40, loss = 0.03844297\n",
      "Iteration 41, loss = 0.05285501\n",
      "Iteration 42, loss = 0.06851370\n",
      "Iteration 43, loss = 0.03260917\n",
      "Iteration 44, loss = 0.04735577\n",
      "Iteration 45, loss = 0.03970009\n",
      "Iteration 46, loss = 0.04515035\n",
      "Iteration 47, loss = 0.03933068\n",
      "Iteration 48, loss = 0.02802041\n",
      "Iteration 49, loss = 0.02969193\n",
      "Iteration 50, loss = 0.04487317\n",
      "Iteration 51, loss = 0.05712144\n",
      "Iteration 52, loss = 0.03329773\n",
      "Iteration 53, loss = 0.03878945\n",
      "Iteration 54, loss = 0.03024666\n",
      "Iteration 55, loss = 0.02219586\n",
      "Iteration 56, loss = 0.03708409\n",
      "Iteration 57, loss = 0.09287871\n",
      "Iteration 58, loss = 0.03399281\n",
      "Iteration 59, loss = 0.02446036\n",
      "Iteration 60, loss = 0.02388917\n",
      "Iteration 61, loss = 0.04740620\n",
      "Iteration 62, loss = 0.03589075\n",
      "Iteration 63, loss = 0.02583492\n",
      "Iteration 64, loss = 0.02792973\n",
      "Iteration 65, loss = 0.03505356\n",
      "Iteration 66, loss = 0.05055423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.42000598\n",
      "Iteration 2, loss = 0.58534523\n",
      "Iteration 3, loss = 0.34831811\n",
      "Iteration 4, loss = 0.25632227\n",
      "Iteration 5, loss = 0.17818980\n",
      "Iteration 6, loss = 0.14121852\n",
      "Iteration 7, loss = 0.12441207\n",
      "Iteration 8, loss = 0.09608950\n",
      "Iteration 9, loss = 0.09871506\n",
      "Iteration 10, loss = 0.08021034\n",
      "Iteration 11, loss = 0.08038702\n",
      "Iteration 12, loss = 0.06684538\n",
      "Iteration 13, loss = 0.06033608\n",
      "Iteration 14, loss = 0.04584819\n",
      "Iteration 15, loss = 0.02198776\n",
      "Iteration 16, loss = 0.04563597\n",
      "Iteration 17, loss = 0.05485225\n",
      "Iteration 18, loss = 0.03353871\n",
      "Iteration 19, loss = 0.01809212\n",
      "Iteration 20, loss = 0.01666169\n",
      "Iteration 21, loss = 0.04707966\n",
      "Iteration 22, loss = 0.03003190\n",
      "Iteration 23, loss = 0.01351644\n",
      "Iteration 24, loss = 0.02504817\n",
      "Iteration 25, loss = 0.02975265\n",
      "Iteration 26, loss = 0.01319951\n",
      "Iteration 27, loss = 0.00518509\n",
      "Iteration 28, loss = 0.01887400\n",
      "Iteration 29, loss = 0.01426561\n",
      "Iteration 30, loss = 0.01366555\n",
      "Iteration 31, loss = 0.01123352\n",
      "Iteration 32, loss = 0.01774312\n",
      "Iteration 33, loss = 0.03652147\n",
      "Iteration 34, loss = 0.01860649\n",
      "Iteration 35, loss = 0.02739055\n",
      "Iteration 36, loss = 0.00703883\n",
      "Iteration 37, loss = 0.03556802\n",
      "Iteration 38, loss = 0.02285062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89148223\n",
      "Iteration 2, loss = 0.30619446\n",
      "Iteration 3, loss = 0.20218581\n",
      "Iteration 4, loss = 0.15094583\n",
      "Iteration 5, loss = 0.12159537\n",
      "Iteration 6, loss = 0.09665680\n",
      "Iteration 7, loss = 0.07505135\n",
      "Iteration 8, loss = 0.08378182\n",
      "Iteration 9, loss = 0.05214438\n",
      "Iteration 10, loss = 0.05116793\n",
      "Iteration 11, loss = 0.04801614\n",
      "Iteration 12, loss = 0.03077473\n",
      "Iteration 13, loss = 0.02915012\n",
      "Iteration 14, loss = 0.05259887\n",
      "Iteration 15, loss = 0.02063670\n",
      "Iteration 16, loss = 0.00869357\n",
      "Iteration 17, loss = 0.02499826\n",
      "Iteration 18, loss = 0.03401889\n",
      "Iteration 19, loss = 0.00655922\n",
      "Iteration 20, loss = 0.00823526\n",
      "Iteration 21, loss = 0.01710664\n",
      "Iteration 22, loss = 0.02173654\n",
      "Iteration 23, loss = 0.02794774\n",
      "Iteration 24, loss = 0.02145541\n",
      "Iteration 25, loss = 0.00764767\n",
      "Iteration 26, loss = 0.01165291\n",
      "Iteration 27, loss = 0.00929222\n",
      "Iteration 28, loss = 0.01207483\n",
      "Iteration 29, loss = 0.00250333\n",
      "Iteration 30, loss = 0.00143439\n",
      "Iteration 31, loss = 0.00146110\n",
      "Iteration 32, loss = 0.02964191\n",
      "Iteration 33, loss = 0.01003454\n",
      "Iteration 34, loss = 0.02091751\n",
      "Iteration 35, loss = 0.00941912\n",
      "Iteration 36, loss = 0.00716459\n",
      "Iteration 37, loss = 0.00272074\n",
      "Iteration 38, loss = 0.00167914\n",
      "Iteration 39, loss = 0.00973105\n",
      "Iteration 40, loss = 0.00256859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.02081318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64569783\n",
      "Iteration 2, loss = 0.19525154\n",
      "Iteration 3, loss = 0.11665288\n",
      "Iteration 4, loss = 0.10497305\n",
      "Iteration 5, loss = 0.07914026\n",
      "Iteration 6, loss = 0.06041410\n",
      "Iteration 7, loss = 0.05762462\n",
      "Iteration 8, loss = 0.04326262\n",
      "Iteration 9, loss = 0.04737415\n",
      "Iteration 10, loss = 0.03314884\n",
      "Iteration 11, loss = 0.04757946\n",
      "Iteration 12, loss = 0.03327780\n",
      "Iteration 13, loss = 0.01811250\n",
      "Iteration 14, loss = 0.03120422\n",
      "Iteration 15, loss = 0.01804151\n",
      "Iteration 16, loss = 0.01323537\n",
      "Iteration 17, loss = 0.03806577\n",
      "Iteration 18, loss = 0.02383775\n",
      "Iteration 19, loss = 0.01208929\n",
      "Iteration 20, loss = 0.00911651\n",
      "Iteration 21, loss = 0.02967406\n",
      "Iteration 22, loss = 0.01383503\n",
      "Iteration 23, loss = 0.02517289\n",
      "Iteration 24, loss = 0.01187401\n",
      "Iteration 25, loss = 0.01099499\n",
      "Iteration 26, loss = 0.00954401\n",
      "Iteration 27, loss = 0.01653739\n",
      "Iteration 28, loss = 0.00554234\n",
      "Iteration 29, loss = 0.01287981\n",
      "Iteration 30, loss = 0.00439588\n",
      "Iteration 31, loss = 0.02594591\n",
      "Iteration 32, loss = 0.01882763\n",
      "Iteration 33, loss = 0.00860278\n",
      "Iteration 34, loss = 0.01278390\n",
      "Iteration 35, loss = 0.01846394\n",
      "Iteration 36, loss = 0.01960643\n",
      "Iteration 37, loss = 0.00246476\n",
      "Iteration 38, loss = 0.00188786\n",
      "Iteration 39, loss = 0.00309581\n",
      "Iteration 40, loss = 0.01689022\n",
      "Iteration 41, loss = 0.00588991\n",
      "Iteration 42, loss = 0.00321592\n",
      "Iteration 43, loss = 0.01681653\n",
      "Iteration 44, loss = 0.00401231\n",
      "Iteration 45, loss = 0.01150999\n",
      "Iteration 46, loss = 0.00191946\n",
      "Iteration 47, loss = 0.00050703\n",
      "Iteration 48, loss = 0.00022266\n",
      "Iteration 49, loss = 0.00013792\n",
      "Iteration 50, loss = 0.00009710\n",
      "Iteration 51, loss = 0.00012691\n",
      "Iteration 52, loss = 0.03986110\n",
      "Iteration 53, loss = 0.01117797\n",
      "Iteration 54, loss = 0.00291051\n",
      "Iteration 55, loss = 0.00219555\n",
      "Iteration 56, loss = 0.00109996\n",
      "Iteration 57, loss = 0.00125802\n",
      "Iteration 58, loss = 0.02038180\n",
      "Iteration 59, loss = 0.00358439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45247055\n",
      "Iteration 2, loss = 0.14946881\n",
      "Iteration 3, loss = 0.12743743\n",
      "Iteration 4, loss = 0.10191714\n",
      "Iteration 5, loss = 0.06432485\n",
      "Iteration 6, loss = 0.06512857\n",
      "Iteration 7, loss = 0.06498501\n",
      "Iteration 8, loss = 0.04540000\n",
      "Iteration 9, loss = 0.04745066\n",
      "Iteration 10, loss = 0.05346265\n",
      "Iteration 11, loss = 0.04536557\n",
      "Iteration 12, loss = 0.03057800\n",
      "Iteration 13, loss = 0.03635920\n",
      "Iteration 14, loss = 0.04153516\n",
      "Iteration 15, loss = 0.02587083\n",
      "Iteration 16, loss = 0.01016910\n",
      "Iteration 17, loss = 0.02054096\n",
      "Iteration 18, loss = 0.01931088\n",
      "Iteration 19, loss = 0.02045900\n",
      "Iteration 20, loss = 0.02358714\n",
      "Iteration 21, loss = 0.03330502\n",
      "Iteration 22, loss = 0.01695913\n",
      "Iteration 23, loss = 0.01126417\n",
      "Iteration 24, loss = 0.01748632\n",
      "Iteration 25, loss = 0.03717306\n",
      "Iteration 26, loss = 0.02865518\n",
      "Iteration 27, loss = 0.02367364\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.17962962962962964, 0.34074074074074073, 0.6444444444444445, 0.6222222222222222, 0.8833333333333333, 0.9166666666666666, 0.9648148148148148, 0.9611111111111111, 0.9685185185185186, 0.9388888888888889]\n"
     ]
    }
   ],
   "source": [
    "valeurs = [1, 2, 3, 4, 5, 10, 25, 50, 100, 200]\n",
    "inter = []\n",
    "result = []\n",
    "\n",
    "for i in range(len(valeurs)):\n",
    "    inter.append(MLPClassifier(hidden_layer_sizes=valeurs[i], activation='tanh', solver='adam', batch_size=1, alpha=0, learning_rate='adaptive', verbose=1))\n",
    "\n",
    "for i in range(len(inter)):\n",
    "    inter[i].fit(X_train, y_train)\n",
    "    result.append(inter[i].score(X_test, y_test))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "plt.plot(np.array(valeurs), np.array(result))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
